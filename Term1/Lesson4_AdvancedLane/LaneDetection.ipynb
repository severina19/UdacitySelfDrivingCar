{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Lane Finding Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image0]: ./output_images/camera_distort.png \"Camera Distortion\"\n",
    "[image1]: ./output_images/Original_Image.png \"Original Image\"\n",
    "[image2]: ./output_images/Undist_Img.png \"Undistorted\"\n",
    "[image3]: ./output_images/binary_warped.png \"Binary Warped Image\"\n",
    "[image4]: ./output_images/color_threshold.png \"Image with Color Threshold\"\n",
    "[image5]: ./output_images/warped_in_b.png \"Warped Image Birds Eye View\"\n",
    "[image6]: ./output_images/result.png \"Output\"\n",
    "[image7]: ./output_images/HLS.png \"HLS color space\"\n",
    "[image8]: ./output_images/image_l.png \"Image in L Space\"\n",
    "[image9]: ./output_images/image_sobel.png \"Image with sobel filter\"\n",
    "[image10]: ./output_images/final_result.png \" Final Result\"\n",
    "\n",
    "[video1]: ./save.mp4 \"Video\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points\n",
    "\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration\n",
    "\n",
    "\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function. A comparision between the original image and the undistorted one is presented in the following pictureï¼š\n",
    "\n",
    "![alt text][image0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline (images)\n",
    "\n",
    "\n",
    "Here I will explain how I have build the image processing pipeline based on one example picture.\n",
    "\n",
    "The example image I have used is shown below.\n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "\n",
    "### image Undistortion\n",
    "The first step of the pipeline is to perform image distortion correction, using the camera matrix we obtained previously. The image with distortion correction is the following:\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "### Color and Gradient Thresholding\n",
    "\n",
    "In the next step  I used a combination of color and gradient thresholds to generate a binary image. I used color transformation to translate the image from rgb space into hls color space. HLS stands for \"Hue\", \"Saturation\" and \"Lightness\", and the color space can be presented by this picture:\n",
    "\n",
    "![alt text][image7]\n",
    "\n",
    "Through converting the image into HSL space I am able to seperate the influence of lighting condition, because the image in L-Space is not dependent on the lightness of the image. The picture in L-Space is shown below:\n",
    "\n",
    "![alt text][image8]\n",
    "\n",
    "Another filter I have used is the sobel operation for finding the gradients in x direction. Because the Lanes will normally have a large gradient in the x-direction, I can find filter out by setting a certain miminal threshold for gradient in x directon. The result I obtain is the following: \n",
    "\n",
    "![alt text][image9]\n",
    "\n",
    "### Perspective Transform\n",
    "The next step to do is to transform the picture into bird eyes view, thus we can calculate the curvature. To do this, I used the warpPerspective function provided by cv2 library. To do that I need to define the source and destination points of the image. I used the ones provided in the template and find the result very satisfying. The values are:\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "| 585, 460      | 320, 0        | \n",
    "| 203, 720      | 320, 720      |\n",
    "| 1127, 720     | 960, 720      |\n",
    "| 695, 460      | 960, 0        |\n",
    "\n",
    "And the warped image in birds eye view becomes:\n",
    "\n",
    "![alt text][image5]\n",
    "\n",
    "Combined with the color and gradient thresholding method, we finally obtain the following image:\n",
    "\n",
    "![alt text][image3]\n",
    "\n",
    "### Polynomial Fit\n",
    "\n",
    "After we obtained the binary warped image, we can starting looking for the lanes. Here I used the sliding window search algorithm introduced in the class. I did this by creating a histogram of the buttom side of the image, and finding maximum values in the left and right halves of the image. The peaks in the histogram will most likely be the x position of the lane. The image is plit into 9 horizontal slices. And starting from the bottom slice, we enclose a 100 pixel wide window around the left peak and right peak of the histogram, and perform mean calculation of all the pixel values. This value will be used as the starting value of the next slice. \n",
    "\n",
    "After obtaining two groups of pixels which will most likely be our left and right lane, we can fit a second order polynomial to each pixel group, which will be our left and right lane. \n",
    "\n",
    "This part is realized by the function 'FitLanes'.\n",
    "\n",
    "\n",
    "### Measuring Curvature and Calculating Offset\n",
    "\n",
    "With the polynomial fit for the left and right lane lines, we can calculated the radius of curvature according to formula introduced in the class, which is also explained [here](http://www.intmath.com/applications-differentiation/8-radius-curvature.php).I converted the distance units from pixels to meters, with the assumption of 30 meters per 720 pixels in the vertical direction, and 3.7 meters per 700 pixels in the horizontal direction. I took the average radius of the left and right lanes and obtained the radius of the lane.\n",
    "\n",
    "Given the polynomial fit for the left and right lane lines, we can calculated the offset between the vehicle and the the lane center. We can make the assuption that the vehicle's center is the center of the image. I calculated the lane's center as the mean x value of the bottom x value of the left lane line, and bottom x value of the right lane line. The offset is then the center of the vehicle substracted by the lane center. \n",
    "\n",
    "The lane radius ad the vehicle's offset from the center are both displayed in the output video stream. This part is realized by the function 'curvature'.\n",
    "\n",
    "\n",
    "### Display Result \n",
    "Now that we have found the lanes and calculated the curvature and the offset, the final step is to warp the image back into the original image and display the results. The output of the image pipeline is shown below. \n",
    "\n",
    "![alt text][image10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline (Video)\n",
    "\n",
    "When we a processing a video, we can use the same pipeline for the image, because a video is only a sequence of many image. But an advange when processing a video is that we can assume that the lanes detected in different image frames are correlating and there will not be large jumpes between two connecting images. \n",
    "\n",
    "To store some information about the lane in the previous running cycles and build up a history of the lane object, I created a class called Line and defined the two attributes \"detected\" and \"last_poly_coeff\". In the attribute \"last_poly_coeff\", the coefficients of the lane in the current picture is stored and used as a starting value of the next picture. As suggsted in the learning material, I also tried out using more attributes such as \"best_fit\" and \"recent_xfitted\", where I store  the polynomial coefficients averaged over the last n iterations, and the x values of the last n fits of the line. These Information can be used when the lane detected in the current picture largely differs from the previous experience value. \n",
    "\n",
    "\n",
    "### Discussion\n",
    "\n",
    "When I first ran my image processing pipeline, the lane detection algorithm was working overall but the lanes are jumping sometimes, despite all the sanity checks I have added. It took me two days until I have detected that the error was already made during the camera calibration step, because I always assumed that my tracking algorithm was wrong. I have falsely put the camera calibration function inside the loop where I read in all the chessboard images for calibration. After I corrected this, The pipeline was working fine for the normal project video, even without any sanity checks. Due to timing reason I was not able to get a perfect line fitting algorithm of the challenging video, but I believe with that with enough sanity checks it will work as well. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image0]: ./output_images/camera_distort.png \"Camera Distortion\"\n",
    "[image1]: ./output_images/Original_Image.png \"Original Image\"\n",
    "[image2]: ./output_images/Undist_Img.png \"Undistorted\"\n",
    "[image3]: ./output_images/binary_warped.png \"Binary Warped Image\"\n",
    "[image4]: ./output_images/color_threshold.png \"Image with Color Threshold\"\n",
    "[image5]: ./output_images/warped_in_b.png \"Warped Image Birds Eye View\"\n",
    "[image6]: ./output_images/result.png \"Output\"\n",
    "[image7]: ./output_images/HLS.png \"HLS color space\"\n",
    "[image8]: ./output_images/image_l.png \"Image in L Space\"\n",
    "[image9]: ./output_images/image_sobel.png \"Image with sobel filter\"\n",
    "[video1]: ./save.mp4 \"Video\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "global counter\n",
    "global lane_left \n",
    "global lane_right\n",
    "global mtx\n",
    "global dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read in images for camera calibration\n",
    "file_path = os.getcwd() +  \"\\\\\"+\"camera_cal\"+ \"\\\\\"\n",
    "images=glob.glob(file_path + \"calibration*.jpg\")\n",
    "\n",
    "objpoints=[]\n",
    "imgpoints=[]\n",
    "nx = 9 # the number of inside corners in x\n",
    "ny = 6 # the number of inside corners in y\n",
    "objp = np.zeros((ny*nx,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1,2)\n",
    "\n",
    "# find object points in chessboard iamge \n",
    "for image in images:\n",
    "    img=cv2.imread(image)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)\n",
    "    if ret==True:\n",
    "        imgpoints.append(corners)\n",
    "        objpoints.append(objp)\n",
    "        img = cv2.drawChessboardCorners(img, (nx,ny), corners, ret)\n",
    "img_size = (gray.shape[1], gray.shape[0])\n",
    "\n",
    "# perform camera calibration to obtain the mtx matrix \n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%matplotlib inline\\nplt.figure(figsize=(10,8))\\n\\nimg = cv2.imread(\"camera_cal/calibration2.jpg\")\\nimg_undist = cv2.undistort(img, mtx, dist, None, mtx)\\nplt.subplot(2,2,1)\\nplt.title(\\'Original Image\\', fontsize=20)\\nfig =plt.imshow(img)\\n\\nplt.subplot(2,2,2)\\nplt.title(\\'Undistorted Image\\', fontsize=20)\\n\\nimg = cv2.imread(\"camera_cal/calibration5.jpg\")\\nimg_undist = cv2.undistort(img, mtx, dist, None, mtx)\\nplt.subplot(2,2,3)\\nplt.title(\\'Original Image\\', fontsize=20)\\n\\n\\nplt.subplot(2,2,4)\\nplt.title(\\'Undistorted Image\\')\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examples of undistorted chessboard image\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "img = cv2.imread(\"camera_cal/calibration2.jpg\")\n",
    "img_undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Original Image', fontsize=20)\n",
    "fig =plt.imshow(img)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Undistorted Image', fontsize=20)\n",
    "\n",
    "img = cv2.imread(\"camera_cal/calibration5.jpg\")\n",
    "img_undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Original Image', fontsize=20)\n",
    "\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Undistorted Image')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.last_poly_coeff = [np.array([False])]  \n",
    "lane_left = Line()\n",
    "lane_right = Line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unwarp(img, src, dst, M_warp):\n",
    "    h,w = img.shape[:2]\n",
    "    warped = cv2.warpPerspective(img, M_warp, (w,h), flags=cv2.INTER_LINEAR)\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sobel_x(img, sobel_kernel=3,min_thres = 20, max_thres =100):\n",
    "\n",
    "    # convert image in rgb to hsl space\n",
    "    img_hsl = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "\n",
    "    # taking the gradient in x and y direction \n",
    "    sobelx1 = cv2.Sobel(img_hsl[:,:,1], cv2.CV_64F, 1,0, ksize=sobel_kernel)\n",
    "    sobelx2 = cv2.Sobel(img_hsl[:,:,2], cv2.CV_64F, 1,0, ksize=sobel_kernel)\n",
    "        \n",
    "    # scaling  \n",
    "    sobelx1 = np.uint8(255*sobelx1/ np.max(sobelx1))\n",
    "    sobelx2 = np.uint8(255*sobelx2/ np.max(sobelx2))\n",
    "\n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "    binary_outputx1 = np.zeros_like(sobelx1)\n",
    "    binary_outputx1[(sobelx1 >= min_thres) & (sobelx1 <= max_thres)] = 1\n",
    "\n",
    "    binary_outputx2 = np.zeros_like(sobelx2)\n",
    "    binary_outputx2[(sobelx2 >= min_thres) & (sobelx2 <= max_thres)] = 1\n",
    "\n",
    "    binary_output = np.zeros_like(sobelx1)\n",
    "    binary_output[(binary_outputx1 ==1) | (binary_outputx2 ==1)]=1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_output_img(undist,M_inv, warped, left_fit, right_fit):\n",
    "\n",
    "    #create output image with lines detected\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    image_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    ploty = np.linspace(0, warped.shape[0]-1, warped.shape[0] )\n",
    "\n",
    "    left_fitx = left_fit[0]*ploty**2+left_fit[1]*ploty+left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2+right_fit[1]*ploty+right_fit[2]\n",
    "\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(image_warp, np.int_([pts]), (50,205,50))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    warp_inv = cv2.warpPerspective(image_warp, M_inv, (image_warp.shape[1], image_warp.shape[0]))\n",
    "    \n",
    "    # Combine the result with the original image\n",
    "    img_out = cv2.addWeighted(undist, 1, warp_inv, 0.3, 0)\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FitLanes(binary_warped):\n",
    "    global counter\n",
    "    global lane_left \n",
    "    global lane_right\n",
    "    \n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    # creating histogram of the image for x axis\n",
    "    histogram = np.sum(binary_warped[int(binary_warped.shape[0]/2):,:], axis=0)\n",
    "\n",
    "       \n",
    "    # find maximal value in the left and right half of the image histogram,\n",
    "    # which most likely represent the position of the left and right lane.\n",
    "    leftx_lane = np.argmax(histogram[:np.int(histogram.shape[0]/2)])\n",
    "    rightx_lane = np.argmax(histogram[np.int(histogram.shape[0]/2):]) + np.int(histogram.shape[0]/2)\n",
    "\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    # width of the searching windows \n",
    "    margin = 100\n",
    "    # minimum number of pixels found in a window\n",
    "    minpix = 50\n",
    "    \n",
    "    if counter>1:\n",
    "        # if we already have a history of the lane, then we can recalculate basd on that history\n",
    "        lft_poly=lane_left.last_poly_coeff\n",
    "        rgt_poly= lane_right.last_poly_coeff\n",
    "        left_lane_inds = ((nonzerox > (lft_poly[0]*(nonzeroy**2) + lft_poly[1]*nonzeroy + lft_poly[2] - margin)) & (nonzerox < (lft_poly[0]*(nonzeroy**2) + lft_poly[1]*nonzeroy + lft_poly[2] + margin))) \n",
    "        right_lane_inds = ((nonzerox > (rgt_poly[0]*(nonzeroy**2) + rgt_poly[1]*nonzeroy + rgt_poly[2] - margin)) & (nonzerox < (rgt_poly[0]*(nonzeroy**2) + rgt_poly[1]*nonzeroy + rgt_poly[2] + margin)))  \n",
    "    else:\n",
    "        left_lane_inds = []\n",
    "        right_lane_inds = []\n",
    "        for window in range(nwindows):\n",
    "\n",
    "            win_y_low = int(binary_warped.shape[0] - (window+1)*window_height)\n",
    "            win_y_high = int(binary_warped.shape[0] - window*window_height)\n",
    "            win_xleft_low = int(leftx_lane - margin)\n",
    "            win_xleft_high = int(leftx_lane + margin)\n",
    "            win_xright_low = int(rightx_lane - margin)\n",
    "            win_xright_high = int(rightx_lane + margin)\n",
    "            # Draw the windows on the visualization image\n",
    "            cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "            cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "            # Identify the nonzero pixels in x and y within the window\n",
    "            good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "            good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "            # Append these indices to the lists\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "            # recenter next window on their mean position\n",
    "            if len(good_left_inds) > minpix:\n",
    "                leftx_lane = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "            if len(good_right_inds) > minpix:        \n",
    "                rightx_lane = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "\n",
    "        # Concatenate the arrays of indices\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    if len(leftx) == 0:\n",
    "        left_fit =[]\n",
    "    else:\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    \n",
    "    if len(rightx) == 0:\n",
    "        right_fit =[]\n",
    "    else:\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    lane_right.last_poly_coeff=right_fit\n",
    "    lane_left.last_poly_coeff = left_fit  \n",
    "\n",
    " \n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "\n",
    "    return left_fit, right_fit,out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_result(binary_warped, left_fit,right_fit,out_img):\n",
    "\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    #plt.figure(figsize=(30,20))\n",
    "    #plt.subplot(2,1,1)\n",
    "    plt.title(\"Binary Warped\", fontsize=20)\n",
    "    plt.imshow(binary_warped, cmap='gray')\n",
    "    #plt.savefig('binary_warped')\n",
    "\n",
    "    #plt.subplot(2,1,2)\n",
    "    binary_warped2 = np.zeros((720, 1280,3))\n",
    "    binary_warped2[:,:,0] = binary_warped\n",
    "    binary_warped2[:,:,1] = binary_warped\n",
    "    binary_warped2[:,:,2] = binary_warped\n",
    "    plt.plot(left_fitx, ploty, color='yellow')\n",
    "    plt.plot(right_fitx, ploty, color='yellow')\n",
    "    result = cv2.addWeighted(binary_warped2, .8, out_img, .8, 0)\n",
    "    plt.title(\"Lanes detected\", fontsize=20)\n",
    "    plt.imshow(result)\n",
    "    #plt.savefig('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def curvature(left_fit, right_fit, binary_warped):\n",
    "    global lane_left \n",
    "    global lane_right\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    \n",
    "    left_curv = ((1 + (2*left_fit[0]*y_eval*ym_per_pix + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curv = ((1 + (2*right_fit[0]*y_eval*ym_per_pix + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "    center = (((left_fit[0]*720**2+left_fit[1]*720+left_fit[2]) +(right_fit[0]*720**2+right_fit[1]*720+right_fit[2]) ) /2 - 750)*xm_per_pix\n",
    "    curv= (left_curv+right_curv)/2\n",
    "\n",
    "    return curv, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def FindLaneInImage(ImgRGB, bVisualize=0):\n",
    "\n",
    "    global counter\n",
    "    global lane_left \n",
    "    global lane_right\n",
    "    global mtx\n",
    "    global dist\n",
    "    \n",
    "    \n",
    "    Img_undistort = cv2.undistort(ImgRGB,mtx,dist,None,mtx)\n",
    "    # Visualize undistortion\n",
    "    if bVisualize ==1:\n",
    "        plt.title('Original Image', fontsize=30)\n",
    "        plt.imshow(ImgRGB)\n",
    "        #plt.savefig('Original_Image.png')\n",
    "        plt.title('Undistorted Image', fontsize=30)\n",
    "        plt.imshow(Img_undistort)\n",
    "        #plt.savefig('Undist_Img.png')\n",
    "    h,w = Img_undistort.shape[:2]\n",
    "    # define source and destination points for transform\n",
    "    left_buttom = (200, 720)\n",
    "    left_top=(585,450)\n",
    "    right_buttom=(1130,720)\n",
    "    right_top=(695,450)\n",
    "\n",
    "    src = np.float32([ left_top,\n",
    "                      left_buttom, \n",
    "                      right_buttom, \n",
    "                      right_top])\n",
    "    \n",
    "\n",
    "    left_buttom = (320, 720)\n",
    "    left_top=(320,0)\n",
    "    right_buttom=(960,720)\n",
    "    right_top=(960,0)\n",
    "    dst = np.float32([left_top, left_buttom, right_buttom, right_top])\n",
    "   \n",
    "\n",
    "    M_warp = cv2.getPerspectiveTransform(src, dst)\n",
    "    M_inv = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "    Img_unwarp = unwarp(Img_undistort, src, dst,M_warp)\n",
    "\n",
    "    # Visualize unwarp\n",
    "    if bVisualize ==1:\n",
    "        plt.title('Warped Image in Birdsview', fontsize=20)\n",
    "        plt.imshow(Img_unwarp)\n",
    "        #plt.savefig('warped_in_b.png')\n",
    "        \n",
    "    image_hls= cv2.cvtColor(Img_undistort, cv2.COLOR_RGB2HSV)\n",
    "    if(bVisualize ==1):\n",
    "        plt.title('HSL Image in H Channel', fontsize=20)\n",
    "        plt.imshow(image_hls[:,:,0],cmap ='gray')\n",
    "        #plt.savefig('image_h.png',cmap ='gray')\n",
    "        plt.title('HSL Image in L Channel', fontsize=20)\n",
    "        plt.imshow(image_hls[:,:,1],cmap ='gray')\n",
    "        #plt.savefig('image_l.png',cmap ='gray')\n",
    "        plt.title('HSL Image in S Channel', fontsize=20)\n",
    "        plt.imshow(image_hls[:,:,2],cmap ='gray')\n",
    "        #plt.savefig('image_s.png',cmap ='gray')\n",
    "    \n",
    "    binary_output_yellow = np.zeros((image_hls.shape[0], image_hls.shape[1]))\n",
    "    binary_output_white = np.zeros((image_hls.shape[0], image_hls.shape[1]))\n",
    "    \n",
    "    binary_output_yellow[(image_hls[:,:,0] >= 0) & (image_hls[:,:,0] <= 50) & (image_hls[:,:,1] >= 100)  & (image_hls[:,:,1] <= 255)  & (image_hls[:,:,2] >= 100) & (image_hls[:,:,2] <= 255)] = 1\n",
    "    binary_output_white[(image_hls[:,:,0] >= 18) & (image_hls[:,:,0] <= 255) & (image_hls[:,:,1] >= 0)  & (image_hls[:,:,1] <= 80)  & (image_hls[:,:,2] >= 180) & (image_hls[:,:,2] <= 255)] = 1\n",
    "\n",
    "    img_color_thresh =np.zeros_like(binary_output_yellow)\n",
    "    img_color_thresh[(binary_output_yellow==1) | (binary_output_white==1)] =1\n",
    "    img_sobelx_thresh = sobel_x(Img_undistort,9,80,220) \n",
    "    if(bVisualize ==1):\n",
    "        plt.title('Image with sobelx filter', fontsize=20)\n",
    "        plt.imshow(img_sobelx_thresh,cmap ='gray')\n",
    "        #plt.savefig('image_h.png',cmap ='gray')\n",
    "        \n",
    "    img_thresh =np.zeros_like(binary_output_yellow)\n",
    "    img_thresh[(binary_output_yellow==1) | (binary_output_white==1) | (img_sobelx_thresh==1)] =1\n",
    "    binary_warped = cv2.warpPerspective(img_thresh, M_warp, (w,h), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    if bVisualize == 1:\n",
    "        plt.title('Image with color threshold', fontsize=20)\n",
    "        plt.imshow(img_color_thresh,cmap ='gray')\n",
    "        #plt.savefig('color_threshold.png')\n",
    "        plt.title('Warped Image with threshold', fontsize=20)\n",
    "        plt.imshow(binary_warped,cmap ='gray')\n",
    "        #plt.savefig('warped_image.png')\n",
    "        \n",
    "\n",
    "    left_fit, right_fit, out_img  = FitLanes(binary_warped)\n",
    "    if bVisualize == 1:\n",
    "        display_result(binary_warped, left_fit,right_fit,out_img)\n",
    "        \n",
    "  \n",
    "    curv, center_off = curvature(left_fit, right_fit, binary_warped)\n",
    "    \n",
    "        #Warp back to original and merge with image    \n",
    "    img_out = create_output_img(Img_undistort, M_inv, binary_warped,left_fit, right_fit)\n",
    "\n",
    "    #Write curvature and center in image\n",
    "    Test_Curv = \"Curvature is: \" + str(int(curv)) + \" m\"\n",
    "    Text_Offset = \"Center offset is: \" + str(round( center_off,2)) + \"m\"\n",
    "    fontScale=1\n",
    "    thickness=2\n",
    "    \n",
    "    fontScale = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img_out,Test_Curv,(10,100),  fontScale, 2,(255,255,255),2,cv2.LINE_AA)\n",
    "    cv2.putText(img_out,Text_Offset,(20,200),  fontScale, 2,(255,255,255),2,cv2.LINE_AA)\n",
    "    counter = counter+1\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path='./test_images/test1.jpg'\n",
    "ImgBGR = cv2.imread(image_path)\n",
    "ImgRGB = cv2.cvtColor(ImgBGR, cv2.COLOR_BGR2RGB)\n",
    "bVisualize=0\n",
    "counter=0\n",
    "img_out=FindLaneInImage(ImgRGB, bVisualize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import moviepy as mve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video 2010.mp4\n",
      "[MoviePy] Writing video 2010.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 50/51 [00:11<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: 2010.mp4 \n",
      "\n",
      "Wall time: 12.1 s\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "#Create video file pipeline\n",
    "counter=0\n",
    "\n",
    "\n",
    "output = '2010.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\").subclip(0,2)\n",
    "\n",
    "out_clip = clip1.fl_image(FindLaneInImage) #NOTE: this function expects color images!!\n",
    "%time out_clip.write_videofile(output, audio=False)\n",
    "print(counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video  width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"2010.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video  width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
