

**Behavioral Cloning Project** 

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image3]: ./images/right_2017_10_05_19_34_46_445.jpg "Right Image"
[image4]: ./images/left_2017_10_05_19_34_46_445.jpg "Left Image"
[image5]: ./images/center_recovering.jpg "center recovering Image"
[image6]: ./images/center_2017_10_05_19_34_46_445.jpg "Center Image"
[image7]: ./images/crop.jpg "Cropped Image"
[image8]: ./images/flip.jpg "Flipped Image"
[image9]: ./images/cnn-architecture.png "NVIDIA Architecture"

## Rubric Points 
###Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
###Files Submitted 

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* writeup_report.md summarizing the results
* run.mp4 containing a video of the vehicle driving autonomously on the track 



###Model Architecture

I used the NVIDIA Model introduced in the course, which requires an input size of 66x200x3. A color space in YUV is recommended and was used in this project.
![image8]
The Architecture of the model is shown in the image above. 

To reduce runtime, I used the lowest resolution and the option "fast" when using the simulator for data generation. The images produced by the simulator have a size of 320x160 with RGB color. This means we need to pre-process the image first and then feed it into the neural network. Detailed information about how the preprocessing was realized is given in the next section.

The input is normalized using the Lambda operation before feeding into the convolution layer. To avoid over-fitting, a drop out layer is introduced before the fully-connected layer. The model includes ELU layers to introduce non-linearity.

The final fully connected layer with 1 output value contains the value of the steering angle.

###Data Generation

Training data was chosen to keep the vehicle driving on the road. To generate smooth data, I used a joystick for measurement generation. The measurement includes some labs where the car was driving in the center of the lane (as good as I could).  Here is an example image of center lane driving:
![image6]
The image capture by the right and the left camera on the same time frame are shown below: 
![image3]
![image4]
I noticed that it is not the best strategy to solely feed the network with data where vehicle is striving perfectly in the middle of the lane, because once the vehicle starts to drive out of the lane the model does not know how to correct. This is why I have also generated some measurement where the vehicle was recovering from the left and right sides of the road. An example of object recovering from the right of the lane back to the middle is shown below:
![image5]
To avoid that the vehicle learns the incorrect behavior of drifting off the road the first place, I paused me recording of that part. 

###Data Preparation

The data generated by the simulator is randomly split into training set and validation set to ensure the model is not over-fitted. I put 20% of the data into validation set. To remove irrelevant information in the image such as sky, cloud etc, I cropped the image as can be seen below:
![image7]

After that, the image is resized to 66x200x3 and converted to YUV color space to fit the NVIDIA Model.

To fully exploit the information in the recorded images, I also used data from the left and right camera, where I adjusted the steering angle with +/- 0.2 degree. Further, to augment the data set, I also flipped the images and 

![image8]
The model used an adam optimizer, so the learning rate was not tuned manually (model.py line 25).


At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road.

